{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report\n",
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#gathering\">Gathering</a></li>\n",
    "<li><a href=\"#assessing\">Assessing</a></li>\n",
    "<li><a href=\"#cleaning\">Cleaning</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gathering'></a>\n",
    "## 1. Gathering\n",
    "> In this part of the project we've been asked to gather three different pieces of data using three different ways:\n",
    "1. Download the twitter_archive_enhanced.csv file which contains The WeRateDogs Twitter archive manually from udacity website\n",
    "2. Download the image_predictions.tsv file programmatically from udacity server using the pythonâ€™s requests library, this file contains The tweet image predictions, i.e., what breed of dog (or other object, animal, etc.)\n",
    "3. This part contains two steps: the first one is to query the twitter API using the tweet IDs in the WeRateDogs Twitter archive then store each tweet's entire set of JSON data in a file called tweet_json.txt, the second is to read this tweet_json.txt file line by line into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assessing'></a>\n",
    "## 2. Assessing\n",
    "> In this part of the project we've been asked to Assess the data obtained in the previous part visually and programmatically for quality and tidiness issues and Detect and document at least eight (8) quality issues and two (2) tidiness issues,\n",
    "i treated every piece of data using pandas's methods like head(), info(), describe(), query(), sum(), isnull(), duplicated(), etc..\n",
    "\n",
    "here are my findings:\n",
    "#### 2.1 Quality\n",
    "##### tweet_api_data\n",
    "- contributors, coordinates, geo, and place columns contain only null values.\n",
    "- favorited, retweeted and truncated columns contain only False values because it depends on the user who gathers the data.\n",
    "- user column contains unrelated data (information about the user who gather the data from the API) and all the rows are the same.\n",
    "- source column: the URL is inside an HTML tag.\n",
    "- created_at data type is a string instead of DateTime.\n",
    "- possibly_sensitive and possibly_sensitive_appealable columns contain only False values.\n",
    "\n",
    "\n",
    "##### image_predictions\n",
    "- p1, p2, p3 columns: whitespace are not standardized sometimes they use '-' other times '_' and some words are Capitalized others not.\n",
    "\n",
    "##### twitter_archive\n",
    "- data type of in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id and retweeted_status_user_id columns is float it should be int.\n",
    "- some names are not real dog names like 'a', 'the', 'an'... (I think that happens because the tweets are not dog rating).\n",
    "- name, doggo, floofer, puppo, and pupper columns contain None string instead of a null value.\n",
    "- The timestamp column data type is a string.\n",
    "- rating_denominator contains some values different than 10.\n",
    "- rating_numerator contains some values less than 10.\n",
    "- rating_numerator data type is int it should float.\n",
    "- rating_numerator contain some values that are incorrectly extracted.\n",
    "- Some tweets are not oginal tweets they are retweets.\n",
    "- extended_entities column contains data that exists in entities column.\n",
    "- id and id_str contain the same data (the same data in different data types).\n",
    "- tweet_api_data and twitter_archive contain duplicated data: created_at, source, text, in_reply_to_status_id, in_reply_to_user_id.\n",
    "\n",
    "#### 2.2 Tidiness\n",
    "- doggo, floofer, pupper and puppo columns should be melted to the same column\n",
    "- tweet_api_data, image_predictions, and twitter_archive should be at the same table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cleaning'></a>\n",
    "## 3. Cleaning\n",
    "In this part, we've been asked to clean each of the issues documented in the previous part.<br>\n",
    "the first thing I have done is to copy the dataframes so I can safely work on the data without changing the original pieces of data, then for each issue, I used 3 steps to clean it:\n",
    "- **Define**: define how I'm going to clean the issue\n",
    "- **Code**: write the necessary code to clean the issue\n",
    "- **Test**: check if the issue is cleaned\n",
    "\n",
    "I've handled tidiness issues first then Quality issues and it is all documented in wrangle_act.ipynb file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
